#!/usr/bin/env python
"""
This is the main program to run Mindboggle.

For help in using Mindboggle ::

    - Online `documentation <http://mindboggle.info/documentation.html>`_
    - README file
    - Help on the command line::

        $ mindboggle --help

This file uses Nipype (http://www.nipy.org/nipype/) to create a workflow
environment to enable Mindboggle to run in a flexible, modular manner
while storing provenance information.

Authors:
    - Arno Klein, 2010-2013  (arno@mindboggle.info)  http://binarybottle.com
    - Satrajit S. Ghosh, 2013  (satra@mit.edu)  http://www.mit.edu/~satra/
    - Each file lists Mindboggle team members who contributed to its content.

Copyright 2013,  Mindboggle team (http://mindboggle.info), Apache v2.0 License

"""

#=============================================================================
#
#   Import libraries
#
#=============================================================================
import os
import sys
import argparse
import warnings
#-----------------------------------------------------------------------------
# Nipype libraries
#-----------------------------------------------------------------------------
from nipype import config, logging
from nipype.pipeline.engine import Workflow, Node, JoinNode
from nipype.interfaces.io import DataGrabber, DataSink
from nipype.interfaces.utility import Function as Fn
from nipype.interfaces.utility import IdentityInterface
from nipype.interfaces.ants import ApplyTransforms
warnings.filterwarnings("ignore")
#-----------------------------------------------------------------------------
# Mindboggle libraries
#-----------------------------------------------------------------------------
from mindboggle.LABELS import DKTprotocol
from mindboggle.DATA import hashes_url
from mindboggle.evaluate.evaluate_labels import measure_surface_overlap, \
    measure_volume_overlap
from mindboggle.features.folds import extract_folds
from mindboggle.features.fundi import extract_fundi, segment_fundi
from mindboggle.features.sulci import extract_sulci
from mindboggle.shapes.laplace_beltrami import spectrum_per_label
from mindboggle.shapes.likelihood import compute_likelihood
from mindboggle.labels.relabel import relabel_surface, relabel_volume, \
    keep_volume_labels, remove_volume_labels, overwrite_volume_labels
from mindboggle.shapes.shape_tools import area, travel_depth, \
    geodesic_depth, curvature
from mindboggle.shapes.zernike.zernike import zernike_moments_per_label
from mindboggle.utils.ants import fetch_ants_data, ComposeMultiTransform, \
    ImageMath, PropagateLabelsThroughMask, fill_volume_with_surface_labels, \
    thickinthehead
from mindboggle.utils.compute import volume_per_label
from mindboggle.utils.freesurfer import surface_to_vtk, curvature_to_vtk, \
    annot_to_vtk, label_with_classifier, convert_mgh_to_native_nifti
from mindboggle.utils.io_table import write_columns, \
    write_shape_stats, write_vertex_measures
from mindboggle.utils.io_uri import retrieve_data
from mindboggle.utils.io_vtk import read_vtk
from mindboggle.utils.mesh import rescale_by_neighborhood
from mindboggle.utils.paths import smooth_skeleton
from mindboggle.utils.segment import split_brain, combine_2labels_in_2volumes
from mindboggle.utils.utils import list_strings

#=============================================================================
#
#   Command-line arguments
#
#=============================================================================
parser = argparse.ArgumentParser(description="""
                    The Mindboggle Python package automates shape analysis of
                    anatomical labels and features extracted from human brain
                    MR image data. For up-to-date documentation, see
                    http://mindboggle.info/documentation.html""",
                     formatter_class = lambda prog:
                     argparse.HelpFormatter(prog, max_help_position=40))
inputs_group = parser.add_argument_group('input arguments')
outputs_group = parser.add_argument_group('output arguments')
features_group = parser.add_argument_group('feature arguments')
shapes_group = parser.add_argument_group('shape measure arguments')
disable_group = parser.add_argument_group('disabling arguments')
extras_group = parser.add_argument_group('bonus arguments')

parser.add_argument("SUBJECT",
                    help=('Subject name corresponding to FreeSurfer subject '
                          'directory'),
                    nargs='+')

parser.add_argument("-v", "--version", help="Version number",
                    action='version', version='%(prog)s 0.1')
parser.add_argument("-n",
                    help=('Number of processors: "-n 1" (default)'),
                    type=int,
                    default=1, metavar='INT')

outputs_group.add_argument("--out",
                    help='Output directory (default: $HOME/mindboggled)',
                    default=os.path.join(os.environ['HOME'],
                                         'mindboggled'), metavar='STR')
outputs_group.add_argument("--working",
                    help='Working directory '
                         '(default: $HOME/mindboggle_working)',
                    default=os.path.join(os.environ['HOME'],
                                         'mindboggle_working'), metavar='STR')
outputs_group.add_argument("--cache", help='Download cache directory '
                                    '(default: $HOME/mindboggle_cache)',
                    default=os.path.join(os.environ['HOME'],
                                         'mindboggle_cache'), metavar='STR')
#inputs.add_argument("--run_freesurfer", action='store_true',
#                    help=("Run recon-all -all to generate expected "
#                          "FreeSurfer files, if not already done."))
#inputs.add_argument("--run_ants", action='store_true',
#                    help=("Run antsCorticalThickness.sh to extract,"
#                          "segment, and register brain (and to provide"
#                          "an additional measure of cortical thickness)"))
inputs_group.add_argument("--freesurfer_data",
                    help=("FreeSurfer subjects directory (default if not "
                          "set: $SUBJECTS_DIR environment variable)"),
                    metavar='STR')
inputs_group.add_argument("--freesurfer_volume",
                    help=("Volume labels: \"wmparc\" (default) or "
                          "\"aparc+aseg\""),
                    choices=['wmparc', 'aparc+aseg'],
                    default='wmparc', metavar='STR')
inputs_group.add_argument("--surface_labels",
                    help=("Surface labels: \"freesurfer\" (default) or "
                          "\"atlas\" (FreeSurfer older than 5.3)"),
                    choices=['freesurfer', 'atlas', 'manual'],
                    default='freesurfer', metavar='STR')
inputs_group.add_argument("--ants_prefix",
                    help=("Prefix string for "
                          "antsCorticalThickness.sh output files: "
                          "\"--ants_prefix /data/ants_output/subject1/ants\""),
                    metavar='STR')
inputs_group.add_argument("--use_segments",
                    help=("Use this file for non/cortex segments"),
                    metavar='STR')
inputs_group.add_argument("--add_atlases", help=("Additional volume atlas(es)"
                                       " in MNI152 space for labeling"),
                    nargs='+', metavar='')

features_group.add_argument("--sulci", action='store_true',
                    help="Extract, identify, and measure sulci")
features_group.add_argument("--fundi", action='store_true',
                    help="Extract, identify, and measure fundi "
                         "[UNDER EVALUATION]")
features_group.add_argument("--vertices", action='store_true',
                    help=("Make table of per-vertex surface shape measures"))

shapes_group.add_argument("--spectra",
                    help='Number of Laplace-Beltrami spectrum eigenvalues '
                         'per label/feature to store in shape tables: '
                         '"--spectra 10" (default is not to run)',
                    default=0, type=int, metavar='INT')
shapes_group.add_argument("--moments",
                    help='Order of Zernike moments per label/feature '
                         'to store in shape tables: "--moments 10" '
                         'is suggested but SLOW (default is not to run) '
                         '[UNDER EVALUATION]',
                    default=0, type=int, metavar='INT')
shapes_group.add_argument("--thickness", action='store_true',
                    help="Compute cortical label thicknesses with "
                         "thickinthehead() [calls ANTs]")

disable_group.add_argument("--no_volumes", action='store_true',
                    help="No volume labels, features, or shape tables")
disable_group.add_argument("--no_surfaces", action='store_true',
                    help="No surface labels, features, or shape tables")
disable_group.add_argument("--no_labels", action='store_true',
                    help="No surface or volume labels")
disable_group.add_argument("--no_shapes", action='store_true',
                    help="No shape tables of surface labels or features")
#disable_group.add_argument("--no_freesurfer_inputs", action='store_true',
#                    help="Don't use FreeSurfer (requires inputs -- UNTESTED)")

extras_group.add_argument("--visual", help=('Generate py/graphviz workflow visual: '
                                      '{hier,flat,exec}'),
                    choices=['hier', 'flat', 'exec'], metavar='STR')
extras_group.add_argument("--cluster", action='store_true',
                    help="Use HTCondor cluster (UNTESTED)")
extras_group.add_argument("--debug", action='store_true',
                    help="Debug nipype with its DEBUG and "
                         "stop_on_first_rerun options")
#extras_group.add_argument("--hashing", action='store_true',
#                    help=("Use content hashing to, for example, "
#                          "replace a file and run downstream of that step"))
args = parser.parse_args()

#-----------------------------------------------------------------------------
# Data arguments:
#-----------------------------------------------------------------------------
#run_freesurfer = args.run_freesurfer
#run_ants = args.run_ants
subject = args.SUBJECT
do_ants = False
freesurfer_data = args.freesurfer_data
if not freesurfer_data:
    freesurfer_data = os.environ['SUBJECTS_DIR']
if args.ants_prefix:
    ants_prefix = args.ants_prefix
    do_ants = True
if args.use_segments:
    use_segments = args.use_segments
else:
    use_segments = None
#-----------------------------------------------------------------------------
# Non-FreeSurfer data arguments:
#-----------------------------------------------------------------------------
do_input_vtk = False  # Load VTK surfaces directly (not FreeSurfer surfaces)
do_input_fs_labels = False  # Load nifti (not FreeSurfer mgh file)
use_FS_inputs = True
no_freesurfer_inputs = False #args.no_freesurfer_inputs
if no_freesurfer_inputs:
    use_FS_inputs = False
    do_input_vtk = True
    do_input_fs_labels = True
#-----------------------------------------------------------------------------
# Label and feature arguments:
#-----------------------------------------------------------------------------
overwrite_cerebrum_with_cerebellum = True
fill_noncortex_with_ants_labels = False
freesurfer_volume = args.freesurfer_volume
surface_labels = args.surface_labels
no_surfaces = args.no_surfaces
no_volumes = args.no_volumes
do_sulci = args.sulci
do_fundi = args.fundi
no_labels = args.no_labels
do_smooth_fundi = False
do_label = False
do_surface = False
do_features = False
do_volumes = False
do_shapes = False
if not no_surfaces:
    do_surface = True
if not no_volumes:
    do_volumes = True
if not no_labels:
    do_label = True
if do_sulci or do_fundi:
    do_features = True
#-----------------------------------------------------------------------------
# Shape arguments:
#-----------------------------------------------------------------------------
no_shapes = args.no_shapes
do_spectra = False  # Measure Laplace-Beltrami spectra for labels/features
do_zernike = False  # Compute Zernike moments for labels/features
if not no_shapes or do_features:
    do_shapes = True
if (do_label or do_features) and do_shapes:
    if args.spectra > 0:
        do_spectra = True
    if args.moments > 0:
        do_zernike = True
do_freesurfer_thickness = False  # Include FreeSurfer's thickness measure
do_freesurfer_convexity = False  # Include FreeSurfer's convexity measure
if do_shapes and use_FS_inputs:
    do_freesurfer_thickness = True
    do_freesurfer_convexity = True
#-----------------------------------------------------------------------------
# Basic functions
#-----------------------------------------------------------------------------
def split_list_pair(List):
    element1 = List[0]
    element2 = List[1]
    return element1, element2

def first_string_containing_substring(substring, List):
    first_matching_string = [x for x in List if substring in x][0]
    return first_matching_string

#=============================================================================
#
#   Hidden arguments: paths, label and template data
#
#=============================================================================
#-----------------------------------------------------------------------------
# Path to C++ code:
#-----------------------------------------------------------------------------
ccode_path = os.environ['MINDBOGGLE_TOOLS']  # Mindboggle C++ code directory
#-----------------------------------------------------------------------------
# Hashes to verify retrieved data, and output, working, and cache directories:
#-----------------------------------------------------------------------------
hashes, url, cache_env, cache = hashes_url()
if args.cache:
    cache = args.cache
elif cache_env in os.environ.keys():
    cache = os.environ[cache_env]
if args.working:
    working = args.working
else:
    working = os.path.join(os.environ['HOME'], 'mindboggle_working')
if not os.path.isdir(args.out):
    print("Create missing output directory: {0}".format(args.out))
    os.makedirs(args.out)
if not os.path.isdir(working):
    print("Create missing working directory: {0}".format(working))
    os.makedirs(working)
if not os.path.isdir(cache):
    print("Create missing cache directory: {0}".format(cache))
    os.makedirs(cache)
#-----------------------------------------------------------------------------
# Labeling protocol information and volume atlases:
#-----------------------------------------------------------------------------
dkt = DKTprotocol()
atlas_volume = 'OASIS-TRT-20_jointfusion_DKT31_CMA_labels_in_MNI152.nii.gz'
add_atlases = args.add_atlases
add_atlas_names = []
if add_atlases:
    if isinstance(add_atlases, str):
        add_atlases = [add_atlases]
    for add_atlas in add_atlases:
        add_atlas_names.append(os.path.basename(add_atlas).split('.')[0])
atropos_to_MNI152_affine = 'OASIS-30_Atropos_template_to_MNI152_affine.txt'
#-----------------------------------------------------------------------------
# Surface atlas labels:
# - 'manual': manual edits
# - FUTURE: <'adjusted': manual edits after automated alignment to fundi>
#-----------------------------------------------------------------------------
surface_classifier = 'DKTatlas40'
surface_atlas_type = 'manual'
modify_surface_labels = False
#-----------------------------------------------------------------------------
# Evaluation
#-----------------------------------------------------------------------------
do_evaluate_surf_labels = False  # Surface overlap: auto vs. manual labels
do_evaluate_vol_labels = False  # Volume overlap: auto vs. manual labels

#=============================================================================
#
#   Initialize workflow inputs and outputs
#
#=============================================================================
mbFlow = Workflow(name='Mindboggle')
mbFlow.base_dir = working
#-----------------------------------------------------------------------------
# Iterate inputs over subjects, hemispheres, and atlases
# (surfaces are assumed to take the form: lh.pial or lh.pial.vtk)
#-----------------------------------------------------------------------------
if add_atlas_names:
    InputAddAtlases = Node(name='Input_volume_atlases',
                           interface=IdentityInterface(fields=['atlas']))
    InputAddAtlases.iterables = ('atlas', add_atlas_names)
InputSubjects = Node(name='Input_subjects',
                     interface=IdentityInterface(fields=['subject']))
InputSubjects.iterables = ('subject', subject)
InputHemis = Node(name='Input_hemispheres',
                  interface=IdentityInterface(fields=['hemi']))
hemis = ['lh', 'rh']
InputHemis.iterables = ('hemi', hemis)
#-----------------------------------------------------------------------------
# Outputs and name substitutions
#-----------------------------------------------------------------------------
Sink = Node(DataSink(), name='Results')
Sink.inputs.base_directory = args.out
Sink.inputs.container = ''

if use_segments:
    seg_in = os.path.basename(use_segments)
else:
    seg_in = 'combined_segmentations.nii.gz'
fs_in = freesurfer_volume + '.nii.gz'
fs_out = 'FreeSurfer_' + freesurfer_volume + '_labels.nii.gz'
fs_filled_in = fs_out +'_to_' + fs_out + '_through_' + seg_in + '_to_' + \
    fs_out + '_through_' + seg_in
fs_filled_out = 'FreeSurfer_' + freesurfer_volume + '_filled_labels.nii.gz'
ants_filled_in = fs_out + '_to_' + \
    'ANTs_labels.nii.gz_to_ANTs_labels.nii.gz_through_' + seg_in
ants_filled_out = 'ANTs_filled_labels.nii.gz'

Sink.inputs.substitutions = [('_subject_', ''),
    ('_hemi_lh', 'left_surface'), ('_hemi_rh', 'right_surface'),
    ('lh.', ''), ('rh.', ''),
    ('pial.', ''),
    ('sulc.vtk', 'FreeSurfer_convexity.vtk'),
    ('thickness.vtk', 'FreeSurfer_thickness.vtk'),
    ('relabeled_aparc.vtk', 'FreeSurfer_cortex_labels.vtk'),
    (fs_in, fs_out),
    (fs_filled_in, fs_filled_out),
    (ants_filled_in, ants_filled_out),
    ('smooth_skeletons.vtk', 'smooth_fundi.vtk')]

Sink.inputs.regexp_substitutions = [
    (r'/_atlas_(.*)/ANTS_labels.nii.gz',
     r'/\1_labels.nii.gz'),
    (r'/_atlas_(.*)/added_atlas_labels_volumes.csv',
     r'/\1_labels_volumes.csv'),
    (r'/left_surface/(.*)/(.*)', r'/\1/left_surface_\2'),
    (r'/right_surface/(.*)/(.*)', r'/\1/right_surface_\2'),
    (r'/features/(.*)/', r'/\1/features/'),
    (r'/labels/(.*)/', r'/\1/labels/'),
    (r'/shapes/(.*)/', r'/\1/shapes/'),
    (r'/tables/(.*)/', r'/\1/tables/')]

#-----------------------------------------------------------------------------
# ANTs transforms for saving MNI152-transformed coordinates and for labeling
#-----------------------------------------------------------------------------
if do_ants:
    #-------------------------------------------------------------------------
    # Retrieve ANTs data:
    #-------------------------------------------------------------------------
    FetchANTs = Node(name='Fetch_ants_data',
                     interface=Fn(function=fetch_ants_data,
                                  input_names=['prefix'],
                                  output_names=['mask',
                                                'segments',
                                                'affine',
                                                'warp',
                                                'invwarp']))
    mbFlow.add_nodes([FetchANTs])
    FetchANTs.inputs.prefix = ants_prefix
    #-------------------------------------------------------------------------
    # Retrieve atlas path:
    #-------------------------------------------------------------------------
    FetchAtlas = Node(name='Fetch_atlas',
                      interface=Fn(function=retrieve_data,
                                   input_names=['data_file',
                                                'url',
                                                'hashes',
                                                'cache_env',
                                                'cache',
                                                'return_missing',
                                                'lookup'],
                                   output_names=['data_path']))
    mbFlow.add_nodes([FetchAtlas])
    FetchAtlas.inputs.data_file = atlas_volume
    FetchAtlas.inputs.url = url
    FetchAtlas.inputs.hashes = hashes
    FetchAtlas.inputs.cache_env = cache_env
    FetchAtlas.inputs.cache = cache
    FetchAtlas.inputs.return_missing = False
    FetchAtlas.inputs.lookup = True
    #-------------------------------------------------------------------------
    # Compose single affine transform from subject to MNI152:
    #-------------------------------------------------------------------------
    if (do_shapes and do_surface) or do_label:
        affine_to_mni = retrieve_data(atropos_to_MNI152_affine,
                                      url, hashes, cache_env, cache)
    if do_shapes and do_surface:
        AffineFileList = Node(name='Merge_affine_file_list',
                              interface=Fn(function=list_strings,
                                           input_names=['string1',
                                                        'string2',
                                                        'string3',
                                                        'string4'],
                                           output_names=['string_list']))
        AffineFileList.inputs.string1 = affine_to_mni
        mbFlow.connect(FetchANTs, 'affine', AffineFileList, 'string2')
        AffineFileList.inputs.string3 = ''
        AffineFileList.inputs.string4 = ''

        ComposeAffine = Node(name='Compose_affine_transform',
                             interface=Fn(function=ComposeMultiTransform,
                                          input_names=['transform_files',
                                                       'inverse_Booleans',
                                                       'output_transform_file',
                                                       'ext'],
                                          output_names=['output_transform_file']))
        mbFlow.add_nodes([ComposeAffine])
        mbFlow.connect(AffineFileList, 'string_list',
                       ComposeAffine, 'transform_files')
        ComposeAffine.inputs.inverse_Booleans = [False, False]
        ComposeAffine.inputs.output_transform_file = ''
        ComposeAffine.inputs.ext = '.txt'
    #-------------------------------------------------------------------------
    # Construct ANTs MNI152-to-subject nonlinear transform lists:
    #-------------------------------------------------------------------------
    if do_label:
        WarpToSubjectFileList = Node(name='Merge_warp_file_list',
                                     interface=Fn(function=list_strings,
                                          input_names=['string1',
                                                       'string2',
                                                       'string3',
                                                       'string4'],
                                          output_names=['string_list']))
        mbFlow.connect(FetchANTs, 'warp', WarpToSubjectFileList, 'string1')
        mbFlow.connect(FetchANTs, 'affine', WarpToSubjectFileList, 'string2')
        WarpToSubjectFileList.inputs.string3 = affine_to_mni
        WarpToSubjectFileList.inputs.string4 = ''
        warp_inverse_Booleans = [False, False, True]

#=============================================================================
#-----------------------------------------------------------------------------
#
#   Surface workflows
#
#-----------------------------------------------------------------------------
#=============================================================================
if do_surface:
    #-------------------------------------------------------------------------
    # Location and structure of the surface inputs:
    #-------------------------------------------------------------------------
    use_white_surface = False
    if use_white_surface:
        Surf = Node(name='Surfaces',
                    interface=DataGrabber(infields=['subject', 'hemi'],
                                          outfields=['surface_files',
                                                     'white_surface_files'],
                                          sort_filelist=False))
    else:
        Surf = Node(name='Surfaces',
                    interface=DataGrabber(infields=['subject', 'hemi'],
                                          outfields=['surface_files'],
                                          sort_filelist=False))
    Surf.inputs.base_directory = freesurfer_data
    Surf.inputs.template = '%s/surf/%s.%s'
    Surf.inputs.template_args['surface_files'] = [['subject', 'hemi', 'pial']]
    if use_white_surface:
        Surf.inputs.template_args['white_surface_files'] = [['subject',
                                                             'hemi', 'white']]
    #Surf.inputs.template_args['sphere_files'] = [['subject','hemi','sphere']]
    if do_freesurfer_thickness:
        Surf.inputs.template_args['freesurfer_thickness_files'] = \
            [['subject', 'hemi', 'thickness']]
    if do_freesurfer_convexity:
        Surf.inputs.template_args['freesurfer_convexity_files'] = \
            [['subject', 'hemi', 'sulc']]

    mbFlow.connect(InputSubjects, 'subject', Surf, 'subject')
    mbFlow.connect(InputHemis, 'hemi', Surf, 'hemi')
    #-------------------------------------------------------------------------
    # Convert surfaces to VTK:
    #-------------------------------------------------------------------------
    if not do_input_vtk:
        ConvertSurf = Node(name='Surface_to_vtk',
                           interface=Fn(function=surface_to_vtk,
                                        input_names=['surface_file',
                                                     'output_vtk'],
                                        output_names=['output_vtk']))
        mbFlow.connect(Surf, 'surface_files', ConvertSurf, 'surface_file')
        ConvertSurf.inputs.output_vtk = ''
        if use_white_surface:
            ConvertWhiteSurf = ConvertSurf.clone('Gray-white_surface_to_vtk')
            mbFlow.add_nodes([ConvertWhiteSurf])
            mbFlow.connect(Surf, 'white_surface_files',
                           ConvertWhiteSurf, 'surface_file')
    #-------------------------------------------------------------------------
    # Evaluation inputs: location and structure of atlas surfaces:
    #-------------------------------------------------------------------------
    if (do_evaluate_surf_labels or surface_labels == 'manual') and do_label:
        SurfaceAtlas = Node(name='Surface_atlas',
                            interface=DataGrabber(infields=['subject','hemi'],
                                                  outfields=['atlas_file'],
                                                  sort_filelist=False))
        SurfaceAtlas.inputs.base_directory = freesurfer_data
        SurfaceAtlas.inputs.template = '%s/label/%s.labels.DKT31.' +\
                                       surface_atlas_type + '.vtk'
        SurfaceAtlas.inputs.template_args['atlas_file'] = [['subject','hemi']]

        mbFlow.connect(InputSubjects, 'subject', SurfaceAtlas, 'subject')
        mbFlow.connect(InputHemis, 'hemi', SurfaceAtlas, 'hemi')

    #=========================================================================
    #
    #   Surface labels
    #
    #=========================================================================
    if do_label:
        SurfLabelFlow = Workflow(name='Surface_labels')

        #=====================================================================
        # Initialize labels with the DKT classifier atlas
        #=====================================================================
        if surface_labels == 'atlas' and use_FS_inputs:
            #-----------------------------------------------------------------
            # Label brain with DKT atlas using FreeSurfer's mris_ca_label:
            #-----------------------------------------------------------------
            Classifier = Node(name='mris_ca_label',
                              interface=Fn(function=label_with_classifier,
                                           input_names=['subject',
                                                        'hemi',
                                                        'left_classifier',
                                                        'right_classifier',
                                                        'annot_file'],
                                           output_names=['annot_file']))
            SurfLabelFlow.add_nodes([Classifier])
            mbFlow.connect(InputSubjects, 'subject',
                           SurfLabelFlow, 'mris_ca_label.subject')
            mbFlow.connect(InputHemis, 'hemi',
                           SurfLabelFlow, 'mris_ca_label.hemi')
            left_classifier_file = 'lh.' + surface_classifier + '.gcs'
            right_classifier_file = 'rh.' + surface_classifier + '.gcs'
            left_classifier = retrieve_data(left_classifier_file, url,
                                            hashes, cache_env, cache)
            right_classifier = retrieve_data(right_classifier_file, url,
                                             hashes, cache_env, cache)
            Classifier.inputs.left_classifier = left_classifier
            Classifier.inputs.right_classifier = right_classifier
            Classifier.inputs.annot_file = ''
            #-----------------------------------------------------------------
            # Convert .annot file to VTK format:
            #-----------------------------------------------------------------
            Classifier2vtk = Node(name='annot_to_vtk',
                                  interface=Fn(function=annot_to_vtk,
                                               input_names=['annot_file',
                                                            'vtk_file'],
                                               output_names=['labels',
                                                             'output_vtk']))
            SurfLabelFlow.add_nodes([Classifier2vtk])
            SurfLabelFlow.connect(Classifier, 'annot_file',
                                  Classifier2vtk, 'annot_file')
            if do_input_vtk:
                mbFlow.connect(Surf, 'surface_files',
                               SurfLabelFlow, 'annot_to_vtk.vtk_file')
            else:
                mbFlow.connect(ConvertSurf, 'output_vtk',
                               SurfLabelFlow, 'annot_to_vtk.vtk_file')
            mbFlow.connect(SurfLabelFlow, 'annot_to_vtk.output_vtk',
                           Sink, 'labels.@DKT_surface')
            plug = 'annot_to_vtk.output_vtk'
            plug1 = Classifier2vtk
            plug2 = 'output_vtk'

        #=====================================================================
        # Initialize labels with FreeSurfer
        #=====================================================================
        elif surface_labels == 'freesurfer' and use_FS_inputs:
            #-----------------------------------------------------------------
            # Location and structure of the FreeSurfer label inputs:
            #-----------------------------------------------------------------
            if use_FS_inputs and do_label and surface_labels == 'freesurfer':
                Annot = Node(name='annot',
                             interface=DataGrabber(infields=['subject',
                                                             'hemi'],
                                                   outfields=['annot_files'],
                                                   sort_filelist=False))
                Annot.inputs.base_directory = freesurfer_data
                Annot.inputs.template = '%s/label/%s.aparc.annot'
                Annot.inputs.template_args['annot_files'] = [['subject',
                                                              'hemi']]
                mbFlow.connect(InputSubjects, 'subject', Annot, 'subject')
                mbFlow.connect(InputHemis, 'hemi', Annot, 'hemi')
            #-----------------------------------------------------------------
            # Convert Annot to VTK format:
            #-----------------------------------------------------------------
            FreeLabels = Node(name='FreeSurfer_annot_to_vtk',
                              interface=Fn(function=annot_to_vtk,
                                           input_names=['annot_file',
                                                        'vtk_file'],
                                           output_names=['labels',
                                                         'output_vtk']))
            SurfLabelFlow.add_nodes([FreeLabels])
            mbFlow.connect(Annot, 'annot_files', SurfLabelFlow,
                           'FreeSurfer_annot_to_vtk.annot_file')
            if do_input_vtk:
                mbFlow.connect(Surf, 'surface_files', SurfLabelFlow,
                               'FreeSurfer_annot_to_vtk.vtk_file')
            else:
                mbFlow.connect(ConvertSurf, 'output_vtk', SurfLabelFlow,
                               'FreeSurfer_annot_to_vtk.vtk_file')
            plug = 'FreeSurfer_annot_to_vtk.output_vtk'
            plug1 = FreeLabels
            plug2 = 'output_vtk'

        #=====================================================================
        # Skip label initialization and process manual (atlas) labels
        #=====================================================================
        elif surface_labels == 'manual':
            ManualSurfLabels = Node(name='Manual_surface_labels',
                                    interface=Fn(function=read_vtk,
                                                 input_names=['input_vtk',
                                                              'return_first',
                                                              'return_array'],
                                                 output_names=['faces',
                                                               'lines',
                                                               'indices',
                                                               'points',
                                                               'npoints',
                                                               'scalars',
                                                               'scalar_names',
                                                               'input_vtk']))
            SurfLabelFlow.add_nodes([ManualSurfLabels])
            mbFlow.connect(SurfaceAtlas, 'atlas_file',
                           SurfLabelFlow, 'Manual_surface_labels.input_vtk')
            ManualSurfLabels.inputs.return_first = 'True'
            ManualSurfLabels.inputs.return_array = 'False'
            plug = 'Manual_surface_labels.input_vtk'
            plug1 = ManualSurfLabels
            plug2 = 'input_vtk'

        ##=====================================================================
        ## Surface label evaluation against manual labels
        ##=====================================================================
        #if do_evaluate_surf_labels:
        #
        #    EvalSurfLabels = Node(name='Evaluate_surface_labels',
        #                          interface=Fn(function=measure_surface_overlap,
        #                                       input_names=['command',
        #                                                    'labels_file1',
        #                                                    'labels_file2'],
        #                                       output_names=['overlap_file']))
        #    mbFlow.add_nodes([EvalSurfLabels])
        #    surface_overlap_command = os.path.join(ccode_path,
        #        'surface_overlap', 'SurfaceOverlapMain')
        #    EvalSurfLabels.inputs.command = surface_overlap_command
        #    mbFlow.connect(SurfaceAtlas, 'atlas_file',
        #                   EvalSurfLabels, 'labels_file1')
        #    mbFlow.connect(SurfLabelFlow, plug,
        #                   'EvalSurfLabels.labels_file2')

        #=====================================================================
        # Convert surface label numbers to volume label numbers
        #=====================================================================
        ReindexLabels = Node(name='Reindex_labels',
                             interface=Fn(function=relabel_surface,
                                          input_names=['vtk_file',
                                                       'hemi',
                                                       'old_labels',
                                                       'new_labels',
                                                       'output_file'],
                                          output_names=['output_file']))
        SurfLabelFlow.add_nodes([ReindexLabels])
        SurfLabelFlow.connect(plug1, plug2, ReindexLabels, 'vtk_file')
        mbFlow.connect(InputHemis, 'hemi',
                       SurfLabelFlow, 'Reindex_labels.hemi')
        ReindexLabels.inputs.old_labels = ''
        ReindexLabels.inputs.new_labels = ''
        ReindexLabels.inputs.output_file = ''
        mbFlow.connect(SurfLabelFlow, 'Reindex_labels.output_file',
                       Sink, 'labels.@surface')

    #=========================================================================
    #
    #   Surface shape measurements
    #
    #=========================================================================
    if do_shapes:
        WholeSurfShapeFlow = Workflow(name='Surface_shapes')
        #---------------------------------------------------------------------
        # Measure surface area:
        #---------------------------------------------------------------------
        SurfaceArea = Node(name='Surface_area',
                    interface=Fn(function=area,
                                 input_names=['command',
                                              'surface_file'],
                                 output_names=['area_file']))
        area_command = os.path.join(ccode_path, 'area', 'PointAreaMain')
        SurfaceArea.inputs.command = area_command
        #---------------------------------------------------------------------
        # Measure surface travel depth:
        #---------------------------------------------------------------------
        TravelDepth = Node(name='Travel_depth',
                           interface=Fn(function=travel_depth,
                                        input_names=['command',
                                                     'surface_file'],
                                        output_names=['depth_file']))
        WholeSurfShapeFlow.add_nodes([TravelDepth])
        TravelDepth.inputs.command = os.path.join(ccode_path,
                                                  'travel_depth',
                                                  'TravelDepthMain')
        #---------------------------------------------------------------------
        # Rescale surface travel depth:
        #---------------------------------------------------------------------
        if do_fundi:
            RescaleTravelDepth = Node(name='Rescale_travel_depth',
                                interface=Fn(function=rescale_by_neighborhood,
                                     input_names=['input_vtk',
                                                  'indices',
                                                  'nedges',
                                                  'p',
                                                  'set_max_to_1',
                                                  'save_file',
                                                  'output_filestring'],
                                     output_names=['rescaled_scalars',
                                                   'rescaled_scalars_file']))
            WholeSurfShapeFlow.add_nodes([RescaleTravelDepth])
            WholeSurfShapeFlow.connect(TravelDepth, 'depth_file',
                                       RescaleTravelDepth, 'input_vtk')
            RescaleTravelDepth.inputs.indices = []
            RescaleTravelDepth.inputs.nedges = 10
            RescaleTravelDepth.inputs.p = 99
            RescaleTravelDepth.inputs.set_max_to_1 = True
            RescaleTravelDepth.inputs.save_file = True
            RescaleTravelDepth.inputs.output_filestring = \
                'travel_depth_rescaled'
        #---------------------------------------------------------------------
        # Measure surface geodesic depth:
        #---------------------------------------------------------------------
        GeodesicDepth = Node(name='Geodesic_depth',
                             interface=Fn(function=geodesic_depth,
                                          input_names=['command',
                                                       'surface_file'],
                                          output_names=['depth_file']))
        GeodesicDepth.inputs.command = os.path.join(ccode_path,
                                                    'geodesic_depth',
                                                    'GeodesicDepthMain')
        #---------------------------------------------------------------------
        # Measure surface curvature:
        #---------------------------------------------------------------------
        CurvNode = Node(name='Curvature',
                        interface=Fn(function=curvature,
                             input_names=['command',
                                          'method',
                                          'arguments',
                                          'surface_file'],
                             output_names=['mean_curvature_file',
                                           'gauss_curvature_file',
                                           'max_curvature_file',
                                           'min_curvature_file',
                                           'min_curvature_vector_file']))
        CurvNode.inputs.command = os.path.join(ccode_path,
                                               'curvature',
                                               'CurvatureMain')
        CurvNode.inputs.method = 2
        CurvNode.inputs.arguments = '-n 0.7'
        #---------------------------------------------------------------------
        # Convert FreeSurfer surface measures to VTK:
        #---------------------------------------------------------------------
        if do_freesurfer_convexity:
            ConvexNode = Node(name='Convexity_to_vtk',
                              interface=Fn(function=curvature_to_vtk,
                                           input_names=['surface_file',
                                                        'vtk_file',
                                                        'output_vtk'],
                                           output_names=['output_vtk']))
            WholeSurfShapeFlow.add_nodes([ConvexNode])
            mbFlow.connect(Surf, 'freesurfer_convexity_files',
                           WholeSurfShapeFlow,
                           'Convexity_to_vtk.surface_file')
            mbFlow.connect(ConvertSurf, 'output_vtk',
                           WholeSurfShapeFlow, 'Convexity_to_vtk.vtk_file')
            ConvexNode.inputs.output_vtk = ''
            mbFlow.connect(WholeSurfShapeFlow, 'Convexity_to_vtk.output_vtk',
                           Sink, 'shapes.@freesurfer_convexity')
        if do_freesurfer_thickness:
            ThickNode = Node(name='Thickness_to_vtk',
                             interface=Fn(function=curvature_to_vtk,
                                          input_names=['surface_file',
                                                       'vtk_file',
                                                       'output_vtk'],
                                          output_names=['output_vtk']))
            WholeSurfShapeFlow.add_nodes([ThickNode])
            mbFlow.connect(Surf, 'freesurfer_thickness_files',
                           WholeSurfShapeFlow,
                           'Thickness_to_vtk.surface_file')
            mbFlow.connect(ConvertSurf, 'output_vtk',
                           WholeSurfShapeFlow, 'Thickness_to_vtk.vtk_file')
            ThickNode.inputs.output_vtk = ''
            mbFlow.connect(WholeSurfShapeFlow, 'Thickness_to_vtk.output_vtk',
                           Sink, 'shapes.@freesurfer_thickness')
        #---------------------------------------------------------------------
        # Connect nodes:
        #---------------------------------------------------------------------
        WholeSurfShapeFlow.add_nodes([SurfaceArea, GeodesicDepth, CurvNode])
        if do_input_vtk:
            mbFlow.connect([(Surf, WholeSurfShapeFlow,
                             [('surface_files','Surface_area.surface_file'),
                              ('surface_files','Travel_depth.surface_file'),
                              ('surface_files','Geodesic_depth.surface_file'),
                              ('surface_files','Curvature.surface_file')])])
        else:
            mbFlow.connect([(ConvertSurf, WholeSurfShapeFlow,
                               [('output_vtk', 'Surface_area.surface_file'),
                                ('output_vtk', 'Travel_depth.surface_file'),
                                ('output_vtk', 'Geodesic_depth.surface_file'),
                                ('output_vtk', 'Curvature.surface_file')])])
        mbFlow.connect([(WholeSurfShapeFlow, Sink,
           [('Surface_area.area_file', 'shapes.@surface_area'),
            ('Travel_depth.depth_file', 'shapes.@travel_depth'),
            ('Geodesic_depth.depth_file', 'shapes.@geodesic_depth'),
            ('Curvature.mean_curvature_file', 'shapes.@mean_curvature')])])

    #=========================================================================
    #
    #   Surface feature extraction
    #
    #=========================================================================
    if do_features:
        SurfFeatureFlow = Workflow(name='Surface_features')

        #=====================================================================
        # Folds and sulci
        #=====================================================================
        if do_sulci:
            #-----------------------------------------------------------------
            # Folds:
            #-----------------------------------------------------------------
            FoldsNode = Node(name='Folds',
                             interface=Fn(function=extract_folds,
                                          input_names=['depth_file',
                                                       'min_fold_size',
                                                       'tiny_depth',
                                                       'save_file'],
                                          output_names=['folds',
                                                        'n_folds',
                                                        'depth_threshold',
                                                        'bins',
                                                        'bin_edges',
                                                        'folds_file']))
            SurfFeatureFlow.add_nodes([FoldsNode])
            mbFlow.connect(WholeSurfShapeFlow, 'Travel_depth.depth_file',
                             SurfFeatureFlow, 'Folds.depth_file')
            FoldsNode.inputs.min_fold_size = 50
            FoldsNode.inputs.tiny_depth = 0.001
            FoldsNode.inputs.save_file = True
            mbFlow.connect(SurfFeatureFlow, 'Folds.folds_file',
                           Sink, 'features.@folds')
            #-----------------------------------------------------------------
            # Sulci:
            #-----------------------------------------------------------------
            SulciNode = Node(name='Sulci',
                             interface=Fn(function=extract_sulci,
                                          input_names=['labels_file',
                                                       'folds_or_file',
                                                       'hemi',
                                                       'min_boundary',
                                                       'sulcus_names'],
                                          output_names=['sulci',
                                                        'n_sulci',
                                                        'sulci_file']))
            SurfFeatureFlow.add_nodes([SulciNode])
            mbFlow.connect(SurfLabelFlow, 'Reindex_labels.output_file',
                           SurfFeatureFlow, 'Sulci.labels_file')
            SurfFeatureFlow.connect(FoldsNode, 'folds',
                                    SulciNode, 'folds_or_file')
            mbFlow.connect(InputHemis, 'hemi', SurfFeatureFlow, 'Sulci.hemi')
            SulciNode.inputs.min_boundary = 1
            SulciNode.inputs.sulcus_names = dkt.sulcus_names
            mbFlow.connect(SurfFeatureFlow, 'Sulci.sulci_file',
                           Sink, 'features.@sulci')

        #=====================================================================
        # Fundi
        #=====================================================================
        if do_fundi:
            #-----------------------------------------------------------------
            # Extract a fundus per fold:
            #-----------------------------------------------------------------
            FoldFundi = Node(name='Fundus_per_fold',
                             interface=Fn(function=extract_fundi,
                                  input_names=['folds',
                                               'curv_file',
                                               'depth_file',
                                               'min_separation',
                                               'erode_ratio',
                                               'erode_min_size',
                                               'save_file'],
                                  output_names=['fundus_per_fold',
                                                'n_fundi_in_folds',
                                                'fundus_per_fold_file']))
            SurfFeatureFlow.connect(FoldsNode, 'folds', FoldFundi, 'folds')
            mbFlow.connect([(WholeSurfShapeFlow, SurfFeatureFlow,
                           [('Curvature.mean_curvature_file',
                             'Fundus_per_fold.curv_file'),
                            ('Rescale_travel_depth.rescaled_scalars_file',
                             'Fundus_per_fold.depth_file')])])
            FoldFundi.inputs.min_separation = 10
            FoldFundi.inputs.erode_ratio = 0.10
            FoldFundi.inputs.erode_min_size = 10
            FoldFundi.inputs.save_file = True
            mbFlow.connect(SurfFeatureFlow,
                           'Fundus_per_fold.fundus_per_fold_file',
                           Sink, 'features.@fundus_per_fold')

            if do_smooth_fundi:
                #-------------------------------------------------------------
                # Compute likelihoods for smoothing fundi:
                #-------------------------------------------------------------
                LikelihoodNode = Node(name='Likelihood',
                    interface=Fn(function=compute_likelihood,
                                 input_names=['trained_file',
                                              'depth_file',
                                              'curvature_file',
                                              'folds',
                                              'save_file'],
                                 output_names=['likelihoods',
                                               'likelihoods_file']))
                SurfFeatureFlow.add_nodes([LikelihoodNode])
                border_params_file = \
                    'depth_curv_border_nonborder_parameters.pkl'
                border_params_path = retrieve_data(border_params_file, url,
                                                   hashes, cache_env, cache)
                LikelihoodNode.inputs.trained_file = border_params_path
                mbFlow.connect([(WholeSurfShapeFlow, SurfFeatureFlow,
                    [('Rescale_travel_depth.rescaled_scalars_file',
                      'Likelihood.depth_file'),
                     ('Curvature.mean_curvature_file',
                      'Likelihood.curvature_file')])])
                SurfFeatureFlow.connect(FoldsNode, 'folds',
                                        LikelihoodNode, 'folds')
                LikelihoodNode.inputs.save_file = True
                #mbFlow.connect(SurfFeatureFlow, 'Likelihood.likelihoods_file',
                #               Sink, 'features.@likelihoods')
                #-------------------------------------------------------------
                # Smooth fundi:
                #-------------------------------------------------------------
                SmoothFundi = Node(name='Smooth_fundi',
                                   interface=Fn(function=smooth_skeleton,
                                        input_names=['skeletons',
                                                     'bounds',
                                                     'vtk_file',
                                                     'likelihoods',
                                                     'wN_max',
                                                     'erode_again',
                                                     'save_file'],
                                        output_names=['smooth_skeletons',
                                                      'n_skeletons',
                                                      'skeletons_file']))
                SurfFeatureFlow.connect(FoldFundi, 'fundus_per_fold',
                                        SmoothFundi, 'skeletons')
                SurfFeatureFlow.connect(FoldsNode, 'folds',
                                        SmoothFundi, 'bounds')
                mbFlow.connect(WholeSurfShapeFlow,
                               'Curvature.mean_curvature_file',
                               SurfFeatureFlow, 'Smooth_fundi.vtk_file')
                SurfFeatureFlow.connect(LikelihoodNode, 'likelihoods',
                                        SmoothFundi, 'likelihoods')
                SmoothFundi.inputs.wN_max = 1.0
                SmoothFundi.inputs.erode_again = False
                SmoothFundi.inputs.save_file = True
                mbFlow.connect(SurfFeatureFlow, 'Smooth_fundi.skeletons_file',
                               Sink, 'features.@smooth_fundi')

            #-----------------------------------------------------------------
            # Segment a fundus per sulcus:
            #-----------------------------------------------------------------
            SulcusFundi = Node(name='Fundus_per_sulcus',
                               interface=Fn(function=segment_fundi,
                                    input_names=['fundus_per_fold',
                                                 'sulci',
                                                 'vtk_file',
                                                 'save_file'],
                                    output_names=['fundus_per_sulcus',
                                                  'n_fundi',
                                                  'fundus_per_sulcus_file']))
            if do_smooth_fundi:
                SurfFeatureFlow.connect(SmoothFundi, 'smooth_skeletons',
                                        SulcusFundi, 'fundus_per_fold')
            else:
                SurfFeatureFlow.connect(FoldFundi, 'fundus_per_fold',
                                        SulcusFundi, 'fundus_per_fold')
            SurfFeatureFlow.connect(SulciNode, 'sulci', SulcusFundi, 'sulci')
            mbFlow.connect(WholeSurfShapeFlow,
                           'Curvature.mean_curvature_file',
                           SurfFeatureFlow, 'Fundus_per_sulcus.vtk_file')
            SulcusFundi.inputs.save_file = True
            mbFlow.connect(SurfFeatureFlow,
                           'Fundus_per_sulcus.fundus_per_sulcus_file',
                           Sink, 'features.@fundus_per_sulcus')

    #=========================================================================
    #
    #   Surface feature shapes
    #
    #=========================================================================
    if do_shapes:
        SurfFeatureShapeFlow = Workflow(name='Surface_feature_shapes')
        #=====================================================================
        # Compute Laplace-Beltrami spectra
        #=====================================================================
        if do_spectra:
            #-----------------------------------------------------------------
            # Measure spectra of labeled regions:
            #-----------------------------------------------------------------
            SpectraLabels = Node(name='Spectra_labels',
                                 interface=Fn(function=spectrum_per_label,
                                              input_names=['vtk_file',
                                                           'spectrum_size',
                                                           'exclude_labels',
                                                           'normalization',
                                                           'area_file',
                                                           'largest_segment'],
                                              output_names=['spectrum_lists',
                                                            'label_list']))
            SurfFeatureShapeFlow.add_nodes([SpectraLabels])
            mbFlow.connect(SurfLabelFlow, 'Reindex_labels.output_file',
                           SurfFeatureShapeFlow, 'Spectra_labels.vtk_file')
            SpectraLabels.inputs.spectrum_size = args.spectra
            SpectraLabels.inputs.exclude_labels = [0]
            SpectraLabels.inputs.normalization = "area"
            SpectraLabels.inputs.area_file = ""
            SpectraLabels.inputs.largest_segment = True
            mbFlow.connect(WholeSurfShapeFlow, 'Surface_area.area_file',
                           SurfFeatureShapeFlow, 'Spectra_labels.area_file')
            #-----------------------------------------------------------------
            # Compute spectra of sulci:
            #-----------------------------------------------------------------
            if do_sulci:
                SpectraSulci = SpectraLabels.clone('Spectra_sulci')
                SurfFeatureShapeFlow.add_nodes([SpectraSulci])
                mbFlow.connect(SurfFeatureFlow, 'Sulci.sulci_file',
                               SurfFeatureShapeFlow, 'Spectra_sulci.vtk_file')
                SpectraSulci.inputs.exclude_labels = [-1]

        #=====================================================================
        # Compute Zernike moments
        #=====================================================================
        if do_zernike:
            #-----------------------------------------------------------------
            # Measure Zernike moments of labeled regions:
            #-----------------------------------------------------------------
            ZernikeLabels = Node(name='Zernike_labels',
                 interface=Fn(function=zernike_moments_per_label,
                              input_names=['vtk_file',
                                           'order',
                                           'exclude_labels',
                                           'scale_input',
                                           'decimate_fraction',
                                           'decimate_smooth'],
                              output_names=['descriptors_lists',
                                            'label_list']))
            SurfFeatureShapeFlow.add_nodes([ZernikeLabels])
            mbFlow.connect(SurfLabelFlow, 'Reindex_labels.output_file',
                           SurfFeatureShapeFlow, 'Zernike_labels.vtk_file')
            ZernikeLabels.inputs.order = args.moments
            ZernikeLabels.inputs.exclude_labels = [0]
            ZernikeLabels.inputs.scale_input = True
            ZernikeLabels.inputs.decimate_fraction = 0
            ZernikeLabels.inputs.decimate_smooth = 0
            #-----------------------------------------------------------------
            # Compute Zernike moments of sulci:
            #-----------------------------------------------------------------
            if do_sulci:
                ZernikeSulci = ZernikeLabels.clone('Zernike_sulci')
                SurfFeatureShapeFlow.add_nodes([ZernikeSulci])
                mbFlow.connect(SurfFeatureFlow, 'Sulci.sulci_file',
                               SurfFeatureShapeFlow, 'Zernike_sulci.vtk_file')
                ZernikeSulci.inputs.exclude_labels = [-1]

    #=========================================================================
    #
    #   Surface feature shape tables
    #
    #=========================================================================
    if do_shapes:
        #---------------------------------------------------------------------
        # Surface feature shape tables: labels, sulci, fundi:
        #---------------------------------------------------------------------
        ShapeTables = Node(name='Shape_tables',
                           interface=Fn(function=write_shape_stats,
                                input_names=['labels_or_file',
                                             'sulci',
                                             'fundi',
                                             'affine_transform_file',
                                             'transform_format',
                                             'area_file',
                                             'mean_curvature_file',
                                             'travel_depth_file',
                                             'geodesic_depth_file',
                                             'freesurfer_convexity_file',
                                             'freesurfer_thickness_file',
                                             'labels_spectra',
                                             'labels_spectra_IDs',
                                             'sulci_spectra',
                                             'sulci_spectra_IDs',
                                             'labels_zernike',
                                             'labels_zernike_IDs',
                                             'sulci_zernike',
                                             'sulci_zernike_IDs',
                                             'exclude_labels',
                                             'delimiter'],
                                output_names=['label_table',
                                              'sulcus_table',
                                              'fundus_table']))
        mbFlow.add_nodes([ShapeTables])
        if do_label:
            mbFlow.connect(SurfLabelFlow, 'Reindex_labels.output_file',
                           ShapeTables, 'labels_or_file')
        else:
            ShapeTables.inputs.labels_or_file = []
        if do_sulci:
            mbFlow.connect(SurfFeatureFlow, 'Sulci.sulci',
                           ShapeTables, 'sulci')
        else:
            ShapeTables.inputs.sulci = []
        if do_fundi:
            mbFlow.connect(SurfFeatureFlow,
                           'Fundus_per_sulcus.fundus_per_sulcus',
                           ShapeTables, 'fundi')
        else:
            ShapeTables.inputs.fundi = []

        if do_ants:
            mbFlow.connect(ComposeAffine, 'output_transform_file',
                           ShapeTables, 'affine_transform_file')
            ShapeTables.inputs.transform_format = 'itk'
        else:
            ShapeTables.inputs.affine_transform_file = None
            ShapeTables.inputs.transform_format = None

        mbFlow.connect([(WholeSurfShapeFlow, ShapeTables,
                           [('Surface_area.area_file',
                             'area_file'),
                            ('Curvature.mean_curvature_file',
                             'mean_curvature_file'),
                            ('Travel_depth.depth_file',
                             'travel_depth_file'),
                            ('Geodesic_depth.depth_file',
                             'geodesic_depth_file')])])
        if do_freesurfer_convexity:
            mbFlow.connect(WholeSurfShapeFlow, 'Convexity_to_vtk.output_vtk',
                           ShapeTables, 'freesurfer_convexity_file')
        else:
            ShapeTables.inputs.freesurfer_convexity_file = ''
        if do_freesurfer_thickness:
            mbFlow.connect(WholeSurfShapeFlow, 'Thickness_to_vtk.output_vtk',
                           ShapeTables, 'freesurfer_thickness_file')
        else:
            ShapeTables.inputs.freesurfer_thickness_file = ''

        # Laplace-Beltrami spectra:
        if do_spectra:
            mbFlow.connect(SurfFeatureShapeFlow,
                           'Spectra_labels.spectrum_lists',
                           ShapeTables, 'labels_spectra')
            mbFlow.connect(SurfFeatureShapeFlow, 'Spectra_labels.label_list',
                           ShapeTables, 'labels_spectra_IDs')
            if do_sulci:
                mbFlow.connect(SurfFeatureShapeFlow,
                               'Spectra_sulci.spectrum_lists',
                               ShapeTables, 'sulci_spectra')
                mbFlow.connect(SurfFeatureShapeFlow,
                               'Spectra_sulci.label_list',
                               ShapeTables, 'sulci_spectra_IDs')
            else:
                ShapeTables.inputs.sulci_spectra = []
                ShapeTables.inputs.sulci_spectra_IDs = []
        else:
            ShapeTables.inputs.labels_spectra = []
            ShapeTables.inputs.sulci_spectra = []
            ShapeTables.inputs.labels_spectra_IDs = []
            ShapeTables.inputs.sulci_spectra_IDs = []

        # Zernike moments:
        if do_zernike:
            mbFlow.connect(SurfFeatureShapeFlow,
                           'Zernike_labels.descriptors_lists',
                           ShapeTables, 'labels_zernike')
            mbFlow.connect(SurfFeatureShapeFlow, 'Zernike_labels.label_list',
                           ShapeTables, 'labels_zernike_IDs')
            if do_sulci:
                mbFlow.connect(SurfFeatureShapeFlow,
                               'Zernike_sulci.descriptors_lists',
                               ShapeTables, 'sulci_zernike')
                mbFlow.connect(SurfFeatureShapeFlow,
                               'Zernike_sulci.label_list',
                               ShapeTables, 'sulci_zernike_IDs')
            else:
                ShapeTables.inputs.sulci_zernike = []
                ShapeTables.inputs.sulci_zernike_IDs = []
        else:
            ShapeTables.inputs.labels_zernike = []
            ShapeTables.inputs.sulci_zernike = []
            ShapeTables.inputs.labels_zernike_IDs = []
            ShapeTables.inputs.sulci_zernike_IDs = []

        ShapeTables.inputs.exclude_labels = [-1]
        ShapeTables.inputs.delimiter = ","
        mbFlow.connect(ShapeTables, 'label_table', Sink, 'tables.@labels')
        if do_sulci:
            mbFlow.connect(ShapeTables, 'sulcus_table', Sink, 'tables.@sulci')
        if do_fundi:
            mbFlow.connect(ShapeTables, 'fundus_table', Sink, 'tables.@fundi')
        #---------------------------------------------------------------------
        # Vertex measures table:
        #---------------------------------------------------------------------
        if args.vertices:
            VertexTable = Node(name='Vertex_table',
                               interface=Fn(function=write_vertex_measures,
                                    input_names=['output_table',
                                                 'labels_or_file',
                                                 'sulci',
                                                 'fundi',
                                                 'affine_transform_file',
                                                 'transform_format',
                                                 'area_file',
                                                 'mean_curvature_file',
                                                 'travel_depth_file',
                                                 'geodesic_depth_file',
                                                 'freesurfer_convexity_file',
                                                 'freesurfer_thickness_file',
                                                 'delimiter'],
                                    output_names=['output_table']))
            mbFlow.add_nodes([VertexTable])
            VertexTable.inputs.output_table = ''
            if do_label:
                mbFlow.connect(SurfLabelFlow, 'Reindex_labels.output_file',
                               VertexTable, 'labels_or_file')
            else:
                VertexTable.inputs.labels_or_file = []
            if do_sulci:
                mbFlow.connect(SurfFeatureFlow, 'Sulci.sulci',
                               VertexTable, 'sulci')
            else:
                VertexTable.inputs.sulci = []
            if do_fundi:
                mbFlow.connect(SurfFeatureFlow,
                               'Fundus_per_sulcus.fundus_per_sulcus',
                               VertexTable, 'fundi')
            else:
                VertexTable.inputs.fundi = []

            if do_ants:
                mbFlow.connect(ComposeAffine, 'output_transform_file',
                               VertexTable, 'affine_transform_file')
                VertexTable.inputs.transform_format = 'itk'
            else:
                VertexTable.inputs.affine_transform_file = None
                VertexTable.inputs.transform_format = None

            mbFlow.connect([(WholeSurfShapeFlow, VertexTable,
                               [('Surface_area.area_file','area_file'),
                                ('Travel_depth.depth_file',
                                 'travel_depth_file'),
                                ('Geodesic_depth.depth_file',
                                 'geodesic_depth_file'),
                                ('Curvature.mean_curvature_file',
                                 'mean_curvature_file')])])
            if do_freesurfer_thickness:
                mbFlow.connect(WholeSurfShapeFlow,
                               'Thickness_to_vtk.output_vtk',
                               VertexTable, 'freesurfer_thickness_file')
            else:
                VertexTable.inputs.freesurfer_thickness_file = ''
            if do_freesurfer_convexity:
                mbFlow.connect(WholeSurfShapeFlow,
                               'Convexity_to_vtk.output_vtk',
                               VertexTable, 'freesurfer_convexity_file')
            else:
                VertexTable.inputs.freesurfer_convexity_file = ''

            VertexTable.inputs.delimiter = ","
            mbFlow.connect(VertexTable, 'output_table',
                           Sink, 'tables.@vertices')

        # #---------------------------------------------------------------------
        # # Apply affine transform to surface coordinates:
        # #---------------------------------------------------------------------
        # TransformPoints = Node(name='Transform_surface_points',
        #                        interface=Fn(function=apply_affine_transform,
        #                                     input_names=['transform_file',
        #                                                  'vtk_or_points',
        #                                                  'transform_format',
        #                                                  'save_file'],
        #                                     output_names=['affine_points',
        #                                                   'output_file']))
        # VolLabelFlow.add_nodes([TransformPoints])
        # if do_ants:
        #     mbFlow.connect(ComposeAffine, 'output_transform_file',
        #                   TransformPoints, 'transform_file')
        # SurfShapeFlow.connect(TravelDepth, 'depth_file',
        #                       TransformPoints, 'vtk_or_points')
        # TransformPoints.inputs.save_file = True
        # mbFlow.connect(SurfShapeFlow, 'Transform_surface_points.output_file',
        #                Sink, 'transforms.@points_to_template')


#=============================================================================
#-----------------------------------------------------------------------------
#
#   Volume workflows
#
#-----------------------------------------------------------------------------
#=============================================================================
if do_volumes and do_label:

    #=========================================================================
    #
    #   Location and structure of FreeSurfer volume inputs
    #
    #=========================================================================
    #-------------------------------------------------------------------------
    # Original image (.mgz) for converting from conformal (below):
    #-------------------------------------------------------------------------
    mghOrig = Node(name='mgh_orig',
                   interface=DataGrabber(infields=['subject'],
                                         outfields=['mgh_orig'],
                                         sort_filelist=False))
    mghOrig.inputs.base_directory = freesurfer_data
    mghOrig.inputs.template = '%s/mri/orig/001.mgz'
    mghOrig.inputs.template_args['mgh_orig'] = [['subject']]
    mbFlow.connect(InputSubjects, 'subject', mghOrig, 'subject')
    #---------------------------------------------------------------------
    # Convert FreeSurfer mgh conformal file to nifti format:
    #---------------------------------------------------------------------
    MGH2Nifti = Node(name='mgh_to_nifti',
                     interface=Fn(function=convert_mgh_to_native_nifti,
                                  input_names=['input_file',
                                               'reference_file',
                                               'output_file',
                                               'interp'],
                                  output_names=['output_file']))
    mbFlow.connect(mghOrig, 'mgh_orig', MGH2Nifti, 'input_file')
    mbFlow.connect(mghOrig, 'mgh_orig', MGH2Nifti, 'reference_file')
    MGH2Nifti.inputs.output_file = ''
    MGH2Nifti.inputs.interp = 'trilin'
    #-------------------------------------------------------------------------
    # Use own whole-brain nifti label volume:
    #-------------------------------------------------------------------------
    if do_input_fs_labels:
        labelsNifti = Node(name='labels_nifti',
                           interface=DataGrabber(infields=['subject'],
                                                 outfields=['labels'],
                                                 sort_filelist=False))
        labelsNifti.inputs.base_directory = freesurfer_data
        labelsNifti.inputs.template = '%s/mri/' + freesurfer_volume+'.nii.gz'
        labelsNifti.inputs.template_args['labels'] = [['subject']]
        mbFlow.connect(InputSubjects, 'subject', labelsNifti, 'subject')
    #-------------------------------------------------------------------------
    # Convert FreeSurfer whole-brain label volume to nifti format:
    #-------------------------------------------------------------------------
    else:
        #---------------------------------------------------------------------
        #  label volume:
        #---------------------------------------------------------------------
        labelsMGH = Node(name='labels_mgh',
                         interface=DataGrabber(infields=['subject'],
                                               outfields=['labels'],
                                               sort_filelist=False))
        labelsMGH.inputs.base_directory = freesurfer_data
        labelsMGH.inputs.template = '%s/mri/' + freesurfer_volume+'.mgz'
        labelsMGH.inputs.template_args['labels'] = [['subject']]
        mbFlow.connect(InputSubjects, 'subject', labelsMGH, 'subject')
        #---------------------------------------------------------------------
        # Convert FreeSurfer mgh conformal file to nifti format:
        #---------------------------------------------------------------------
        labelsMGH2Nifti = Node(name='labels_mgh_to_nifti',
                           interface=Fn(function=convert_mgh_to_native_nifti,
                                        input_names=['input_file',
                                                     'reference_file',
                                                     'output_file',
                                                     'interp'],
                                        output_names=['output_file']))
        mbFlow.connect(labelsMGH, 'labels', labelsMGH2Nifti, 'input_file')
        mbFlow.connect(mghOrig, 'mgh_orig', labelsMGH2Nifti, 'reference_file')
        labelsMGH2Nifti.inputs.output_file = ''
        labelsMGH2Nifti.inputs.interp = 'nearest'
        mbFlow.connect(labelsMGH2Nifti, 'output_file',
                       Sink, 'labels.@freesurfer')

    #=========================================================================
    #
    #   Volume labels
    #
    #=========================================================================
    VolLabelFlow = Workflow(name='Volume_labels')

    #---------------------------------------------------------------------
    # Extract FreeSurfer cerebellum labels:
    #---------------------------------------------------------------------
    FScerebellum = Node(name='Extract_FreeSurfer_cerebella',
                        interface=Fn(function=keep_volume_labels,
                                     input_names=['input_file',
                                                  'labels_to_keep',
                                                  'output_file',
                                                  'second_file'],
                                     output_names=['output_file']))
    VolLabelFlow.add_nodes([FScerebellum])
    if do_input_fs_labels:
        mbFlow.connect(labelsNifti, 'labels', VolLabelFlow,
                       'Extract_FreeSurfer_cerebella.input_file')
    else:
        mbFlow.connect(labelsMGH2Nifti, 'output_file', VolLabelFlow,
                       'Extract_FreeSurfer_cerebella.input_file')
    FScerebellum.inputs.labels_to_keep = dkt.cerebellum_numbers
    FScerebellum.inputs.output_file = ''
    FScerebellum.inputs.second_file = ''

    #=========================================================================
    # Combine FreeSurfer and ANTs cerebrum segmentation volumes
    #=========================================================================
    if not use_segments:
        #---------------------------------------------------------------------
        # Extract FreeSurfer cerebrum labels:
        #---------------------------------------------------------------------
        FScerebrum = Node(name='Extract_FreeSurfer_cerebra',
                          interface=Fn(function=keep_volume_labels,
                                       input_names=['input_file',
                                                    'labels_to_keep',
                                                    'output_file',
                                                    'second_file'],
                                       output_names=['output_file']))
        VolLabelFlow.add_nodes([FScerebrum])
        if do_input_fs_labels:
            mbFlow.connect(labelsNifti, 'labels', VolLabelFlow,
                           'Extract_FreeSurfer_cerebra.input_file')
        else:
            mbFlow.connect(labelsMGH2Nifti, 'output_file', VolLabelFlow,
                           'Extract_FreeSurfer_cerebra.input_file')
        labels_to_segment = dkt.cerebrum_cortex_numbers + \
                            dkt.cerebrum_noncortex_numbers + \
                            dkt.brainstem_numbers + \
                            dkt.extra_numbers
        FScerebrum.inputs.labels_to_keep = labels_to_segment
        FScerebrum.inputs.output_file = ''
        FScerebrum.inputs.second_file = ''
        #---------------------------------------------------------------------
        # Convert FreeSurfer cerebrum labels to non/cortex segments:
        #---------------------------------------------------------------------
        FSsegments = Node(name='FreeSurfer_cerebrum_labels_to_segments',
                          interface=Fn(function=relabel_volume,
                                       input_names=['input_file',
                                                    'old_labels',
                                                    'new_labels',
                                                    'output_file'],
                                       output_names=['output_file']))
        VolLabelFlow.add_nodes([FSsegments])
        VolLabelFlow.connect(FScerebrum, 'output_file',
                             FSsegments, 'input_file')
        FSsegments.inputs.old_labels = labels_to_segment
        FSsegments.inputs.new_labels = \
            [2 for x in dkt.cerebrum_cortex_numbers] + \
            [3 for x in dkt.cerebrum_noncortex_numbers] + \
            [3 for x in dkt.brainstem_numbers] + \
            [3 for x in dkt.extra_numbers]
        FSsegments.inputs.output_file = ''
        #---------------------------------------------------------------------
        # Convert ANTs Atropos-segmented volume to non/cortex segments:
        #---------------------------------------------------------------------
        if do_ants:
            ANTsSegments = FSsegments.clone('Relabel_ANTs_segments')
            VolLabelFlow.add_nodes([ANTsSegments])
            mbFlow.connect(FetchANTs, 'segments', VolLabelFlow,
                           'Relabel_ANTs_segments.input_file')
            ANTsSegments.inputs.old_labels = [1, 4]
            ANTsSegments.inputs.new_labels = [0, 3]
            ANTsSegments.inputs.output_file = ''
            #-----------------------------------------------------------------
            # Combine FreeSurfer and ANTs cerebrum segmentation volumes to
            # obtain a single cortex (2) and noncortex (3) segmentation file:
            #-----------------------------------------------------------------
            JoinSegs = Node(name='Combine_FreeSurfer_ANTs_cerebrum_segments',
                            interface=Fn(function=combine_2labels_in_2volumes,
                                         input_names=['file1',
                                                      'file2',
                                                      'label1',
                                                      'label2',
                                                      'output_file'],
                                         output_names=['output_file']))
            VolLabelFlow.add_nodes([JoinSegs])
            VolLabelFlow.connect(FSsegments, 'output_file', JoinSegs, 'file1')
            JoinSegs.inputs.out_dir = ''
            VolLabelFlow.connect(ANTsSegments, 'output_file',
                                 JoinSegs, 'file2')
            JoinSegs.inputs.label1 = 3
            JoinSegs.inputs.label2 = 2
            JoinSegs.inputs.output_file = ''
            #-----------------------------------------------------------------
            # Erase cerebrum that overlaps with FreeSurfer cerebellum:
            #-----------------------------------------------------------------
            if overwrite_cerebrum_with_cerebellum:
                RemoveCerebellum = Node(
                    name='Remove_cerebrum_cerebellum_overlap',
                    interface=Fn(function=remove_volume_labels,
                                 input_names=['input_file',
                                              'labels_to_remove',
                                              'output_file',
                                              'second_file'],
                                 output_names=['output_file']))
                VolLabelFlow.add_nodes([RemoveCerebellum])
                VolLabelFlow.connect(FScerebellum, 'output_file',
                                     RemoveCerebellum, 'input_file')
                RemoveCerebellum.inputs.labels_to_remove = \
                    dkt.cerebellum_numbers
                RemoveCerebellum.inputs.output_file = ''
                VolLabelFlow.connect(JoinSegs, 'output_file',
                                     RemoveCerebellum, 'second_file')

    #=========================================================================
    # Split segmented brain into two sides (without medial regions)
    #=========================================================================
    if modify_surface_labels:
        #---------------------------------------------------------------------
        # Split brain by masking with left or right labels:
        #---------------------------------------------------------------------
        SplitBrainSegs = Node(name='Split_brain',
                              interface=Fn(function=split_brain,
                                           input_names=['image_file',
                                                        'label_file',
                                                        'left_labels',
                                                        'right_labels'],
                                           output_names=['left_brain',
                                                         'right_brain']))
        VolLabelFlow.add_nodes([SplitBrainSegs])
        if use_segments:
            SplitBrainSegs.inputs.image_file = use_segments
        else:
            if do_ants:
                if overwrite_cerebrum_with_cerebellum:
                    VolLabelFlow.connect(RemoveCerebellum, 'output_file',
                                         SplitBrainSegs, 'image_file')
                else:
                    VolLabelFlow.connect(JoinSegs, 'output_file',
                                         SplitBrainSegs, 'image_file')
            else:
                VolLabelFlow.connect(FSsegments, 'output_file',
                                     SplitBrainSegs, 'image_file')
        if do_input_fs_labels:
            mbFlow.connect(labelsNifti, 'labels', VolLabelFlow,
                           'Split_brain.label_file')
        else:
            mbFlow.connect(labelsMGH2Nifti, 'output_file', VolLabelFlow,
                           'Split_brain.label_file')
        SplitBrainSegs.inputs.left_labels = dkt.left_cerebrum_numbers
        SplitBrainSegs.inputs.right_labels = dkt.right_cerebrum_numbers

    #=========================================================================
    # Fill cerebrum segmentation volumes with FreeSurfer labels
    #=========================================================================
    #-------------------------------------------------------------------------
    # Extract cerebrum noncortical volume labels:
    #-------------------------------------------------------------------------
    FSnoncortex = Node(name='Extract_FreeSurfer_noncortex_labels',
                       interface=Fn(function=keep_volume_labels,
                                    input_names=['input_file',
                                                 'labels_to_keep',
                                                 'output_file',
                                                 'second_file'],
                                    output_names=['output_file']))
    VolLabelFlow.add_nodes([FSnoncortex])
    if do_input_fs_labels:
        mbFlow.connect(labelsNifti, 'labels', VolLabelFlow,
                       'Extract_FreeSurfer_noncortex_labels.input_file')
    else:
        mbFlow.connect(labelsMGH2Nifti, 'output_file', VolLabelFlow,
                       'Extract_FreeSurfer_noncortex_labels.input_file')
    labels_to_fill = dkt.cerebrum_noncortex_numbers + \
                     dkt.brainstem_numbers + \
                     dkt.extra_numbers
    FSnoncortex.inputs.labels_to_keep = labels_to_fill
    FSnoncortex.inputs.output_file = ''
    FSnoncortex.inputs.second_file = ''
    #-------------------------------------------------------------------------
    # Propagate FreeSurfer volume labels through noncortex:
    #-------------------------------------------------------------------------
    FSFillNoncortex = Node(name='Fill_noncortex_with_FreeSurfer_labels',
                           interface=Fn(function=PropagateLabelsThroughMask,
                                        input_names=['mask',
                                                     'labels',
                                                     'mask_index',
                                                     'output_file',
                                                     'binarize',
                                                     'stopvalue'],
                                        output_names=['output_file']))
    VolLabelFlow.add_nodes([FSFillNoncortex])
    if use_segments:
        FSFillNoncortex.inputs.mask = use_segments
    else:
        if do_ants:
            if overwrite_cerebrum_with_cerebellum:
                VolLabelFlow.connect(RemoveCerebellum, 'output_file',
                                     FSFillNoncortex, 'mask')
            else:
                VolLabelFlow.connect(JoinSegs, 'output_file',
                                     FSFillNoncortex, 'mask')
        else:
            VolLabelFlow.connect(FSsegments, 'output_file',
                                 FSFillNoncortex, 'mask')
    VolLabelFlow.connect(FSnoncortex, 'output_file',
                         FSFillNoncortex, 'labels')
    FSFillNoncortex.inputs.mask_index = 3
    FSFillNoncortex.inputs.output_file = ''
    FSFillNoncortex.inputs.binarize = False
    FSFillNoncortex.inputs.stopvalue = ''
    #-------------------------------------------------------------------------
    # Propagate FreeSurfer surface labels through whole-brain cortex:
    #-------------------------------------------------------------------------
    if do_surface and modify_surface_labels:

        print('NOTE: Reevaluate surface-to-volume label propagation '
              'when surface label modification algorithm complete.')

        #---------------------------------------------------------------------
        # Propagate surface labels through each hemisphere's cortex:
        #---------------------------------------------------------------------
        FSsurfFillCortex = Node(
                    name='Fill_cortex_with_FreeSurfer_surface_labels',
                    interface=Fn(function=fill_volume_with_surface_labels,
                                 input_names=['hemi',
                                              'left_mask',
                                              'right_mask',
                                              'surface_files',
                                              'mask_index',
                                              'output_file',
                                              'binarize'],
                                 output_names=['output_file']))
        VolLabelFlow.add_nodes([FSsurfFillCortex])
        mbFlow.connect(InputHemis, 'hemi', VolLabelFlow,
                       'Fill_cortex_with_FreeSurfer_surface_labels.hemi')
        VolLabelFlow.connect(SplitBrainSegs, 'left_brain',
                             FSsurfFillCortex, 'left_mask')
        VolLabelFlow.connect(SplitBrainSegs, 'right_brain',
                             FSsurfFillCortex, 'right_mask')
        mbFlow.connect(SurfLabelFlow, 'Reindex_labels.output_file',
            VolLabelFlow,
            'Fill_cortex_with_FreeSurfer_surface_labels.surface_files')
        FSsurfFillCortex.inputs.mask_index = 2
        FSsurfFillCortex.inputs.output_file = ''
        FSsurfFillCortex.inputs.binarize = False
        #---------------------------------------------------------------------
        # Combine left and right cortical labels:
        #---------------------------------------------------------------------
        SplitHemiList = JoinNode(name='Split_hemisphere_list',
                                 interface=Fn(function=split_list_pair,
                                              input_names=['List'],
                                              output_names=['element1',
                                                            'element2']),
                                 joinsource="Input_hemispheres",
                                 joinfield="List")
        LRcortex = Node(name='Combine_left_right_cortex_labels',
                        interface=Fn(function=ImageMath,
                                     input_names=['volume1',
                                                  'volume2',
                                                  'operator',
                                                  'output_file'],
                                     output_names=['output_file']))
        VolLabelFlow.add_nodes([SplitHemiList, LRcortex])
        VolLabelFlow.connect(FSsurfFillCortex, 'output_file',
                             SplitHemiList, 'List')
        VolLabelFlow.connect(SplitHemiList, 'element1', LRcortex, 'volume1')
        VolLabelFlow.connect(SplitHemiList, 'element2', LRcortex, 'volume2')
        LRcortex.inputs.operator = '+'
        LRcortex.inputs.output_file = ''
    #-------------------------------------------------------------------------
    # Propagate FreeSurfer volume labels through whole-brain cortex:
    #-------------------------------------------------------------------------
    else:
        #---------------------------------------------------------------------
        # Extract FreeSurfer cerebrum cortical volume labels:
        #---------------------------------------------------------------------
        FScortex = FSnoncortex.clone('Extract_FreeSurfer_cortex_labels')
        VolLabelFlow.add_nodes([FScortex])
        if do_input_fs_labels:
            mbFlow.connect(labelsNifti, 'labels', VolLabelFlow,
                           'Extract_FreeSurfer_cortex_labels.input_file')
        else:
            mbFlow.connect(labelsMGH2Nifti, 'output_file', VolLabelFlow,
                           'Extract_FreeSurfer_cortex_labels.input_file')
        FScortex.inputs.labels_to_keep = dkt.cerebrum_cortex_numbers
        FScortex.inputs.output_file = ''
        FScortex.inputs.second_file = ''

        #---------------------------------------------------------------------
        # Propagate volume labels through whole-brain cortex:
        #---------------------------------------------------------------------
        FSFillCortex = FSFillNoncortex.clone(
                            'Fill_cortex_with_FreeSurfer_labels')
        VolLabelFlow.add_nodes([FSFillCortex])
        if use_segments:
            FSFillCortex.inputs.mask = use_segments
        else:
            if do_ants:
                if overwrite_cerebrum_with_cerebellum:
                    VolLabelFlow.connect(RemoveCerebellum, 'output_file',
                                         FSFillCortex, 'mask')
                else:
                    VolLabelFlow.connect(JoinSegs, 'output_file',
                                         FSFillCortex, 'mask')
            else:
                VolLabelFlow.connect(FSsegments, 'output_file',
                                     FSFillCortex, 'mask')
        VolLabelFlow.connect(FScortex, 'output_file',
                             FSFillCortex, 'labels')
        FSFillCortex.inputs.mask_index = 2
    #-------------------------------------------------------------------------
    # Combine FreeSurfer label-filled whole-brain cortex and noncortex:
    #-------------------------------------------------------------------------
    CombineFSLabels = Node(name=
                           'Combine_FreeSurfer_cortex_noncortex_labels',
                           interface=Fn(function=overwrite_volume_labels,
                                        input_names=['source',
                                                     'target',
                                                     'output_file',
                                                     'ignore_labels',
                                                     'erase_labels'],
                                        output_names=['output_file']))
    VolLabelFlow.add_nodes([CombineFSLabels])
    VolLabelFlow.connect(FSFillNoncortex, 'output_file',
                         CombineFSLabels, 'source')
    if modify_surface_labels:
        VolLabelFlow.connect(LRcortex, 'output_file',
                             CombineFSLabels, 'target')
    else:
        VolLabelFlow.connect(FSFillCortex, 'output_file',
                             CombineFSLabels, 'target')
    CombineFSLabels.inputs.output_file = ''
    CombineFSLabels.inputs.ignore_labels = [0]
    CombineFSLabels.inputs.erase_labels = False
    if not overwrite_cerebrum_with_cerebellum:
        mbFlow.connect(VolLabelFlow,
               'Combine_FreeSurfer_cortex_noncortex_labels.output_file',
               Sink, 'labels.@freesurfer_filled')

    #=========================================================================
    # Fill whole-brain segmentation volumes with ANTs labels
    #=========================================================================
    if do_ants:
        #---------------------------------------------------------------------
        # Mask brain volume:
        #---------------------------------------------------------------------
        #MaskBrain = Node(name= 'Mask_brain',
        #                 interface=Fn(function=ImageMath,
        #                              input_names=['volume1',
        #                                           'volume2',
        #                                           'operator',
        #                                           'output_file'],
        #                              output_names=['output_file']))
        #VolLabelFlow.add_nodes([MaskBrain])
        #mbFlow.connect(MGH2Nifti, 'output_file',
        #               VolLabelFlow, 'Mask_brain.volume1')
        #mbFlow.connect(FetchANTs, 'mask', VolLabelFlow, 'Mask_brain.volume2')
        #MaskBrain.inputs.operator = 'm'
        #MaskBrain.inputs.output_file = ''
        #---------------------------------------------------------------------
        # Transform default atlas labels in MNI152 to subject via template:
        #---------------------------------------------------------------------
        xfm = Node(ApplyTransforms(), name='antsApplyTransforms')
        VolLabelFlow.add_nodes([xfm])
        xfm.inputs.dimension = 3
        xfm.inputs.default_value = 0
        xfm.inputs.interpolation = 'NearestNeighbor'
        xfm.inputs.invert_transform_flags = warp_inverse_Booleans
        xfm.inputs.output_image = 'ANTs_labels.nii.gz'
        mbFlow.connect(FetchANTs, 'segments', VolLabelFlow,
                       'antsApplyTransforms.reference_image')
        mbFlow.connect(FetchAtlas, 'data_path',
                       VolLabelFlow, 'antsApplyTransforms.input_image')
        mbFlow.connect(WarpToSubjectFileList, 'string_list', VolLabelFlow,
                       'antsApplyTransforms.transforms')
        mbFlow.connect(VolLabelFlow, 'antsApplyTransforms.output_image',
                       Sink, 'labels.@antsRegistration')
        #---------------------------------------------------------------------
        # Extract ANTs cerebral cortical volume labels:
        #---------------------------------------------------------------------
        ANTsCortex = FSnoncortex.clone('Extract_ANTs_cortex_labels')
        VolLabelFlow.add_nodes([ANTsCortex])
        VolLabelFlow.connect(xfm, 'output_image', ANTsCortex, 'input_file')
        ANTsCortex.inputs.labels_to_keep = dkt.cerebrum_cortex_numbers
        ANTsCortex.inputs.output_file = ''
        ANTsCortex.inputs.second_file = ''
        #---------------------------------------------------------------------
        # Extract ANTs whole-brain noncortical volume labels:
        #---------------------------------------------------------------------
        ANTsNoncortex = ANTsCortex.clone('Extract_ANTs_noncortex_labels')
        VolLabelFlow.add_nodes([ANTsNoncortex])
        VolLabelFlow.connect(xfm, 'output_image', ANTsNoncortex, 'input_file')
        ANTsNoncortex.inputs.labels_to_keep = labels_to_fill
        ANTsNoncortex.inputs.output_file = ''
        #---------------------------------------------------------------------
        # Propagate ANTs whole-brain cortical volume labels through cortex:
        #---------------------------------------------------------------------
        ANTsFillCortex = FSFillNoncortex.clone('Fill_cortex_with_ANTs_labels')
        VolLabelFlow.add_nodes([ANTsFillCortex])
        if use_segments:
            ANTsFillCortex.inputs.mask = use_segments
        else:
            if overwrite_cerebrum_with_cerebellum:
                VolLabelFlow.connect(RemoveCerebellum, 'output_file',
                                     ANTsFillCortex, 'mask')
            else:
                VolLabelFlow.connect(JoinSegs, 'output_file',
                                     ANTsFillCortex, 'mask')
        VolLabelFlow.connect(ANTsCortex, 'output_file',
                             ANTsFillCortex, 'labels')
        ANTsFillCortex.inputs.mask_index = 2
        ANTsFillCortex.inputs.output_file = ''
        ANTsFillCortex.inputs.binarize = False
        ANTsFillCortex.inputs.stopvalue = ''
        #---------------------------------------------------------------------
        # Propagate ANTs whole-brain noncortical labels through noncortex:
        #---------------------------------------------------------------------
        if fill_noncortex_with_ants_labels:
            ANTsFillNoncortex = ANTsFillCortex.clone(
                'Fill_noncortex_with_ANTs_labels')
            VolLabelFlow.add_nodes([ANTsFillNoncortex])
            if use_segments:
                ANTsFillNoncortex.inputs.mask = use_segments
            else:
                if overwrite_cerebrum_with_cerebellum:
                    VolLabelFlow.connect(RemoveCerebellum, 'output_file',
                                         ANTsFillNoncortex, 'mask')
                else:
                    VolLabelFlow.connect(JoinSegs, 'output_file',
                                         ANTsFillNoncortex, 'mask')
            VolLabelFlow.connect(ANTsNoncortex, 'output_file',
                                 ANTsFillNoncortex, 'labels')
            ANTsFillNoncortex.inputs.mask_index = 3
        #---------------------------------------------------------------------
        # Combine ANTs label-filled cortex and label-filled noncortex:
        #---------------------------------------------------------------------
        CombineANTsLabels = CombineFSLabels.clone(
            'Combine_ANTs_cortex_noncortex_labels')
        VolLabelFlow.add_nodes([CombineANTsLabels])
        if fill_noncortex_with_ants_labels:
            VolLabelFlow.connect(ANTsFillNoncortex, 'output_file',
                                 CombineANTsLabels, 'source')
        else:
            VolLabelFlow.connect(ANTsNoncortex, 'output_file',
                                 CombineANTsLabels, 'source')
        VolLabelFlow.connect(ANTsFillCortex, 'output_file',
                             CombineANTsLabels, 'target')
        CombineANTsLabels.inputs.output_file = ''
        CombineANTsLabels.inputs.ignore_labels = [0]
        CombineANTsLabels.inputs.erase_labels = False
        if not overwrite_cerebrum_with_cerebellum:
            mbFlow.connect(VolLabelFlow, 
                           'Combine_ANTs_cortex_noncortex_labels.output_file',
                           Sink, 'labels.@ants_filled')

    #=========================================================================
    # Add FreeSurfer cerebellum labels
    #=========================================================================
    #-------------------------------------------------------------------------
    # ...to FreeSurfer cerebrum labels:
    #-------------------------------------------------------------------------
    AddFScerebellum = CombineFSLabels.clone(
        'FreeSurfer_cerebrum_cerebellum')
    VolLabelFlow.add_nodes([AddFScerebellum])
    VolLabelFlow.connect(FScerebellum, 'output_file',
                         AddFScerebellum, 'source')
    VolLabelFlow.connect(CombineFSLabels, 'output_file',
                         AddFScerebellum, 'target')
    AddFScerebellum.inputs.output_file = ''
    AddFScerebellum.inputs.ignore_labels = [0]
    AddFScerebellum.inputs.erase_labels = False
    mbFlow.connect(VolLabelFlow,
           'FreeSurfer_cerebrum_cerebellum.output_file',
           Sink, 'labels.@freesurfer_filled_and_cerebellum')
    #-------------------------------------------------------------------------
    # ...to ANTs cerebrum labels:
    #-------------------------------------------------------------------------
    if do_ants:
        AddFScerebellum2ANTs = AddFScerebellum.clone(
            'FreeSurfer_cerebellum_ANTs_cerebrum')
        VolLabelFlow.add_nodes([AddFScerebellum2ANTs])
        VolLabelFlow.connect(FScerebellum, 'output_file',
                             AddFScerebellum2ANTs, 'source')
        VolLabelFlow.connect(CombineANTsLabels, 'output_file',
                             AddFScerebellum2ANTs, 'target')
        AddFScerebellum2ANTs.inputs.output_file = ''
        AddFScerebellum2ANTs.inputs.ignore_labels = [0]
        AddFScerebellum2ANTs.inputs.erase_labels = False
        mbFlow.connect(VolLabelFlow,
               'FreeSurfer_cerebellum_ANTs_cerebrum.output_file',
               Sink, 'labels.@ants_filled_and_cerebellum')

    #=========================================================================
    # Transform labels from added atlas(es) in MNI152 to subject
    #=========================================================================
    if do_ants and add_atlas_names:
        #---------------------------------------------------------------------
        # Find atlas path that contains atlas name:
        #---------------------------------------------------------------------
        MatchAtlas = Node(name='Match_added_atlas',
                      interface=Fn(function=first_string_containing_substring,
                                   input_names=['substring',
                                                'List'],
                                   output_names=['first_matching_string']))
        VolLabelFlow.add_nodes([MatchAtlas])
        mbFlow.connect(InputAddAtlases, 'atlas', VolLabelFlow,
                       'Match_added_atlas.substring')
        MatchAtlas.inputs.List = add_atlases
        #---------------------------------------------------------------------
        # Transform atlas:
        #---------------------------------------------------------------------
        xfm2 = xfm.clone('Transform_added_atlases')
        VolLabelFlow.add_nodes([xfm2])
        xfm2.inputs.output_image = 'ANTs_added_atlas_labels.nii.gz'
        VolLabelFlow.connect(MatchAtlas, 'first_matching_string',
                             xfm2, 'input_image')
        mbFlow.connect(FetchANTs, 'segments', VolLabelFlow,
                       'Transform_added_atlases.reference_image')
        mbFlow.connect(WarpToSubjectFileList, 'string_list', VolLabelFlow,
                       'Transform_added_atlases.transforms')
        mbFlow.connect(VolLabelFlow, 'Transform_added_atlases.output_image',
                       Sink, 'labels.@added_atlases')

    ##=========================================================================
    ## Evaluate label volume overlaps
    ##=========================================================================
    #if do_evaluate_vol_labels:
    #
    #    #---------------------------------------------------------------------
    #    # Evaluation inputs: location and structure of atlas volumes
    #    #---------------------------------------------------------------------
    #    VolAtlas = Node(name='Volume_atlas',
    #                    interface=DataGrabber(infields=['subject'],
    #                                          outfields=['atlas_vol_file'],
    #                                          sort_filelist=False))
    #    VolLabelFlow.add_nodes([VolAtlas])
    #    VolAtlas.inputs.base_directory = freesurfer_data
    #    VolAtlas.inputs.template = '%s/mri/labels.DKT31.manual.nii.gz'
    #    VolAtlas.inputs.template_args['atlas_vol_file'] = [['subject']]
    #    mbFlow.connect(InputSubjects, 'subject',
    #                   VolLabelFlow, 'Volume_atlas.subject')
    #    #---------------------------------------------------------------------
    #    # Evaluate volume labels
    #    #---------------------------------------------------------------------
    #    EvalVolLabels = Node(name='Evaluate_volume_labels',
    #                         interface=Fn(function=measure_volume_overlap,
    #                                      input_names=['labels',
    #                                                   'file2',
    #                                                   'file1'],
    #                                      output_names=['overlaps',
    #                                                    'out_file']))
    #    VolLabelFlow.add_nodes([EvalVolLabels])
    #    EvalVolLabels.inputs.labels = dkt.label_numbers
    #    VolLabelFlow.connect(VolAtlas, 'atlas_vol_file',
    #                         EvalVolLabels, 'file2')
    #    if do_ants:
    #        VolLabelFlow.connect(ANTSwFSg, 'output_file',
    #                             EvalVolLabels, 'file1')
    #    else:
    #        VolLabelFlow.connect(CombineFSLabels, 'output_file',
    #                             EvalVolLabels, 'file1')

    #=========================================================================
    #
    #   Volume feature shapes
    #
    #=========================================================================
    if do_shapes:

        VolShapeFlow = Workflow(name='Volume_feature_shapes')

        FSVolTable = Node(name='FreeSurfer_filled_label_volume_table',
                          interface=Fn(function=write_columns,
                                       input_names=['columns',
                                                    'column_names',
                                                    'delimiter',
                                                    'quote',
                                                    'input_table',
                                                    'output_table'],
                                       output_names=['output_table']))
        VolShapeFlow.add_nodes([FSVolTable])
        FSVolTable.inputs.delimiter = ','
        FSVolTable.inputs.quote = True
        FSVolTable.inputs.input_table = ''
        FSVolTable.inputs.output_table = ''

        #=====================================================================
        # Measure volume of each region of a labeled image file
        #=====================================================================
        #---------------------------------------------------------------------
        # Volumes of the FreeSurfer filled labels:
        #---------------------------------------------------------------------
        FSlabelVolumes = Node(name='FreeSurfer_filled_label_volumes',
                              interface=Fn(function=volume_per_label,
                                           input_names=['labels',
                                                        'input_file'],
                                           output_names=['labels_volumes']))
        VolShapeFlow.add_nodes([FSlabelVolumes])
        FSlabelVolumes.inputs.labels = dkt.label_numbers
        mbFlow.connect(VolLabelFlow,
               'Combine_FreeSurfer_cortex_noncortex_labels.output_file',
               VolShapeFlow, 'FreeSurfer_filled_label_volumes.input_file')
        # Table:
        VolShapeFlow.connect(FSlabelVolumes, 'labels_volumes',
                             FSVolTable, 'columns')
        FSVolTable.inputs.column_names = ['FreeSurfer_filled_labels',
                                          'volumes']
        mbFlow.connect(VolShapeFlow,
                       'FreeSurfer_filled_label_volume_table.output_table',
                       Sink, 'tables.@volumes_of_FreeSurfer_labels')
        if do_ants:
            #-----------------------------------------------------------------
            # Volumes of the ANTs filled labels:
            #-----------------------------------------------------------------
            antsLabelVolumes = FSlabelVolumes.clone(
                'ANTs_filled_label_volumes')
            VolShapeFlow.add_nodes([antsLabelVolumes])
            if overwrite_cerebrum_with_cerebellum:
                mbFlow.connect(VolLabelFlow,
                       'FreeSurfer_cerebellum_ANTs_cerebrum.output_file',
                       VolShapeFlow, 'ANTs_filled_label_volumes.input_file')
            else:
                mbFlow.connect(VolLabelFlow,
                       'Combine_ANTs_cortex_noncortex_labels.output_file',
                       VolShapeFlow, 'ANTs_filled_label_volumes.input_file')
            # Table:
            antsVolTable = FSVolTable.clone('ANTs_filled_label_volume_table')
            VolShapeFlow.add_nodes([antsVolTable])
            antsVolTable.inputs.column_names = ['ANTs_filled_labels',
                                                'volumes']
            VolShapeFlow.connect(antsLabelVolumes, 'labels_volumes',
                                 antsVolTable, 'columns')
            mbFlow.connect(VolShapeFlow,
                           'ANTs_filled_label_volume_table.output_table',
                           Sink, 'tables.@volumes_of_ANTs_labels')
            #-----------------------------------------------------------------
            # Volumes of labels in additional atlases transformed to subject:
            #-----------------------------------------------------------------
            if add_atlas_names:
                antsLabelVolumes2 = FSlabelVolumes.clone(
                    'Added_atlas_label_volumes')
                VolShapeFlow.add_nodes([antsLabelVolumes2])
                mbFlow.connect(VolLabelFlow,
                               'Transform_added_atlases.output_image',
                               VolShapeFlow,
                               'Added_atlas_label_volumes.input_file')
                # Table:
                antsVolTable2 = FSVolTable.clone(
                    'Added_atlas_label_volume_table')
                VolShapeFlow.add_nodes([antsVolTable2])
                antsVolTable2.inputs.column_names = ['added_atlas_labels',
                                                     'volumes']
                VolShapeFlow.connect(antsLabelVolumes2, 'labels_volumes',
                                     antsVolTable2, 'columns')
                mbFlow.connect(VolShapeFlow,
                               'Added_atlas_label_volume_table.output_table',
                               Sink, 'tables.@volumes_of_added_atlas_labels')

        #=====================================================================
        # Measure volume, thickness of cortical regions of labeled image file
        #=====================================================================
        if args.thickness:
            #-----------------------------------------------------------------
            # Thicknesses of the FreeSurfer cortical labels:
            #-----------------------------------------------------------------
            FSthicknesses = Node(
                name='FreeSurfer_filled_cortex_label_thicknesses',
                interface=Fn(function=thickinthehead,
                             input_names=['segmented_file',
                                          'labeled_file',
                                          'cortex_value',
                                          'noncortex_value',
                                          'labels',
                                          'resize',
                                          'propagate',
                                          'output_dir',
                                          'output_table'],
                             output_names=['label_volume_area_thickness',
                                           'thickness_table']))
            VolShapeFlow.add_nodes([FSthicknesses])
            if use_segments:
                FSthicknesses.inputs.segmented_file = use_segments
            else:
                if do_ants:
                    if overwrite_cerebrum_with_cerebellum:
                        VolLabelFlow.connect(RemoveCerebellum, 'output_file',
                                             FSthicknesses, 'segmented_file')
                    else:
                        VolLabelFlow.connect(JoinSegs, 'output_file',
                                             FSthicknesses, 'segmented_file')
                else:
                    VolLabelFlow.connect(FSsegments, 'output_file',
                                         FSthicknesses, 'segmented_file')
            mbFlow.connect(VolLabelFlow,
                'Combine_FreeSurfer_cortex_noncortex_labels.output_file',
                VolShapeFlow,
                'FreeSurfer_filled_cortex_label_thicknesses.labeled_file')
            FSthicknesses.inputs.cortex_value = 2
            FSthicknesses.inputs.noncortex_value = 3
            FSthicknesses.inputs.labels = dkt.cerebrum_cortex_numbers
            FSthicknesses.inputs.resize = True
            FSthicknesses.inputs.propagate = False
            FSthicknesses.inputs.output_dir = ''
            FSthicknesses.inputs.output_table = ''
            FSthicknesses.inputs.use_c3d = False
            # Table:
            FSThickTable = FSVolTable.clone(
                'FreeSurfer_filled_cortex_label_thickness_table')
            VolShapeFlow.add_nodes([FSThickTable])
            VolShapeFlow.connect(FSthicknesses,
                                 'label_volume_area_thickness',
                                 FSThickTable, 'columns')
            FSThickTable.inputs.column_names = ['FreeSurfer_filled_labels',
                                                'volumes', 'areas',
                                                'thicknesses']
            mbFlow.connect(VolShapeFlow,
            'FreeSurfer_filled_cortex_label_thickness_table.output_table',
            Sink, 'tables.@thicknesses_of_FreeSurfer_labels')
            #-----------------------------------------------------------------
            # Thicknesses of the ANTs cortical labels:
            #-----------------------------------------------------------------
            if do_ants:
                ANTsThicknesses = FSthicknesses.\
                    clone('ANTs_filled_cortex_label_thicknesses')
                VolShapeFlow.add_nodes([ANTsThicknesses])
                if use_segments:
                    ANTsThicknesses.inputs.segmented_file = use_segments
                else:
                    if overwrite_cerebrum_with_cerebellum:
                        VolLabelFlow.connect(RemoveCerebellum, 'output_file',
                                             ANTsThicknesses, 'segmented_file')
                    else:
                        VolLabelFlow.connect(JoinSegs, 'output_file',
                                             ANTsThicknesses, 'segmented_file')
                if overwrite_cerebrum_with_cerebellum:
                    mbFlow.connect(VolLabelFlow,
                       'FreeSurfer_cerebellum_ANTs_cerebrum.output_file',
                       VolShapeFlow,
                       'ANTs_filled_cortex_label_thicknesses.labeled_file')
                else:
                    mbFlow.connect(VolLabelFlow,
                    'Combine_ANTs_cortex_noncortex_labels.output_file',
                    VolShapeFlow,
                    'ANTs_filled_cortex_label_thicknesses.labeled_file')
                # Table:
                ANTsThickTable = FSVolTable.clone(
                    'ANTs_filled_cortex_label_thickness_table')
                VolShapeFlow.add_nodes([ANTsThickTable])
                VolShapeFlow.connect(ANTsThicknesses,
                                     'label_volume_area_thickness',
                                     ANTsThickTable, 'columns')
                ANTsThickTable.inputs.column_names = ['ANTs_filled_labels',
                                                      'volumes', 'areas',
                                                      'thicknesses']
                mbFlow.connect(VolShapeFlow,
                   'ANTs_filled_cortex_label_thickness_table.output_table',
                   Sink, 'tables.@thicknesses_of_ANTs_filled_cortex_labels')


#=============================================================================
#-----------------------------------------------------------------------------
#
#   Run workflows
#
#-----------------------------------------------------------------------------
#=============================================================================
if __name__ == '__main__':

    from time import time
    time0 = time()

    #-------------------------------------------------------------------------
    # Set the workflow configuration to use content hashing:
    #-------------------------------------------------------------------------
    mbFlow.config['execution']['hash_method'] = 'content'
    #hashing = args.hashing
    #if hashing:
    #mbFlow.config['execution']['hash_method'] = 'content'
    #mbFlow.config['execution']['use_relative_paths'] = True

    #-------------------------------------------------------------------------
    # Generate a visual graph:
    #-------------------------------------------------------------------------
    graph_vis = args.visual
    if graph_vis == 'hier':
        graph_vis = 'hierarchical'
    if graph_vis:
        if graph_vis == 'exec':
            mbFlow.write_graph(graph2use=graph_vis, simple_form=False)
        else:
            mbFlow.write_graph(graph2use=graph_vis)

    #-------------------------------------------------------------------------
    # Debug: http://nipy.org/nipype/users/config_file.html#debug-configuration
    #-------------------------------------------------------------------------
    if args.debug:
        config.set('logging', 'workflow_level', 'DEBUG')
        logging.update_logging(config)
        mbFlow.config['execution']['stop_on_first_rerun'] = True

    #-------------------------------------------------------------------------
    # Run (HTCondor) cluster processes, such as on the Mindboggler cluster:
    #-------------------------------------------------------------------------
    if args.cluster:
        mbFlow.run(plugin='CondorDAGMan')
    #-------------------------------------------------------------------------
    # Run multiple processes or not:
    #-------------------------------------------------------------------------
    else:
        if args.n:
            if args.n > 1:
                mbFlow.run(plugin='MultiProc',
                           plugin_args={'n_procs': args.n})
                           #updatehash=True)
            else:
                mbFlow.run()  #updatehash=True)
        else:
            mbFlow.run()  #updatehash=True)
        # # Default is to use all processors:
        #else:
        #    mbFlow.run(plugin='MultiProc')

    print('Mindboggle run finished ({0:0.2f} seconds).'.format(time() - time0))
